# -*- coding: utf-8 -*-
"""Final One

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_cwhpRb-wRRfGOYE-vnnH7q9gXDPeujk
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#With this backend, the output of plotting commands is displayed inline within frontends like the Jupyter notebook, 
#directly below the code cell that produced it. The resulting plots will then also be stored in the notebook document.
# %matplotlib inline
import seaborn as sns
from scipy.stats import kurtosis

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from mlxtend.frequent_patterns import apriori, association_rules 

import tensorflow as tf

import pandas as pd
from collections import defaultdict

"""**Converting data to Binary Data:**"""

data=pd.read_csv('d.csv')
data

data.isna().sum()

{column: len(data[column].unique()) for column in data.select_dtypes('object').columns}

{column: list(data[column].unique()) for column in data.select_dtypes('object').columns}

def encode_gender(x):
    if x.lower()[0] == 'f':
        return 0
    elif x.lower()[0] == 'm':
        return 1
    else:
        return 2

data['Gender'] = data['Gender'].apply(encode_gender)

data

target='Have you ever thought of attempting suicide?'


binary_features = [
    'friends/Relatives',
    'understanding parents',
    'injury',
    'fights',
    'bullied',
    'toxic relationship',
    'stressed',
    'missed classes',
    'smoke',
    'Alcohol',
    'got drunk',
    'drugs',
    'bad experience'

]

ordinal_features = [
    'Weight'
]

def binary_encode(df, columns, positive_values):
    df = df.copy()
    for column, positive_value in zip(columns, positive_values):
        df[column] = df[column].apply(lambda x: 1 if x == positive_value else 0)
    return df

def ordinal_encode(df, columns, orderings):
    df = df.copy()
    for column, ordering in zip(columns, orderings):
        df[column] = df[column].apply(lambda x: ordering.index(x))
    return df

binary_positive_values = ['Yes' for feature in binary_features]

ordinal_orderings = [
    ['More than 70', '50 - 60', '60 - 70', '50-60']
]

data = binary_encode(
    data,
    columns=binary_features,
    positive_values=binary_positive_values
)
data = ordinal_encode(
    data,
    columns=ordinal_features,
    orderings=ordinal_orderings
)

data

data = binary_encode(data, columns=['Have you ever thought of attempting suicide?'], positive_values=['Yes'])

y = data['Have you ever thought of attempting suicide?'].copy()
X = data.drop('Have you ever thought of attempting suicide?', axis=1).copy()

scaler = StandardScaler()

X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=100)

X.shape

print("Class Distribution (Positive to Negative): {:.1f}% / {:.1f}%".format(y_train.mean() * 100, (1 - y_train.mean()) * 100))

pca = PCA(n_components=2)

pca.fit(X)

x_pca=pca.transform(X)

X.shape

x_pca.shape

X

x_pca

pca.explained_variance_ratio_

pca.n_components_

pca = PCA().fit(X)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

X_train_pca, X_test_pca, y_train,y_test=train_test_split(x_pca,y,test_size=0.2,random_state=30)

"""**DIkSHA**"""

df_orig = pd.read_csv('/content/sample_data/data.csv')
df = df_orig.copy()
df.info()

df.shape

df.head()

df.describe()

df.columns.duplicated()

df.isnull().sum()

bp = df.boxplot(column=['smoke', 'bullied'])

df_num = df.select_dtypes(exclude=object)
for col in df_num.columns:
  fig, ax = plt.subplots(1, 2, figsize=(16,5))
  sns.histplot(data=df_num, x=col, ax=ax[0])
  sns.boxplot(data=df_num, x=col, ax=ax[1])

for col in df_num.columns:
  sk = df_num[col].skew(axis = 0, skipna = True)
  if (sk > 0.5) or (sk< -0.5):
    print(col," with ",sk," is positively skewed! ")
  else:
    print(col, " with ", sk, " is normal!")

for col in df_num.columns:
  kt = kurtosis(df_num[col], axis=0, bias=True)
  if kt > 3:
    print(col," with ",kt," is Leptokurtic! Higher chance of outliers!")
  elif kt < 3:
    print(col," with ",kt," is Platykurtic! Medium chance of outliers!")
  else:
    print(col," with ",kt," is Mesokurtic! No chance of outliers!")

df_cat = df.select_dtypes(include=object)
for col in df_cat.columns:
  fig, ax = plt.subplots(1, 1, figsize=(16,5))
  df_cat[col].value_counts(normalize=True).plot.bar(figsize = (24,6), title=col)

for col in df_num.columns:
  fig, ax = plt.subplots(1,figsize=(16,5))
  plt.scatter(df[col], df['attempted_suicide'])
  plt.xlabel(col)
  plt.ylabel('attempted_suicide')

for col in df_cat.columns:
  df.boxplot(column = 'attempted_suicide', by = col,figsize=(25,6))

max(df['attempted_suicide'])

med = df['attempted_suicide'].median()

df['attempted_suicide'].replace(1,med,inplace=True)

max(df['attempted_suicide'])

med = df['attempted_suicide'].median()

df['attempted_suicide'].replace(0,med,inplace=True)

fig, ax = plt.subplots(1, 2, figsize=(16,5))
sns.histplot(data=df, x=df['attempted_suicide'], ax=ax[0])
sns.boxplot(data=df, x=df['attempted_suicide'], ax=ax[1])

max(df['Alcohol'])

df['attempted_suicide'].replace(1,med,inplace=True)

fig, ax = plt.subplots(1,figsize=(16,5))
plt.scatter(df['Alcohol'], df['attempted_suicide'])
plt.xlabel('Alcohol')
plt.ylabel('attempted_suicide')

fig, ax = plt.subplots(1, 2, figsize=(16,5))
sns.histplot(data=df, x=df['Alcohol'], ax=ax[0])
sns.boxplot(data=df, x=df['Alcohol'], ax=ax[1])

max(df['injury'])

med = df['injury'].median()
df['injury'].replace(1,med,inplace=True)

fig, ax = plt.subplots(1,figsize=(16,5))
plt.scatter(df['injury'], df['attempted_suicide'])
plt.xlabel('injury')
plt.ylabel('attempted_suicide')

fig, ax = plt.subplots(1, 2, figsize=(16,5))
sns.histplot(data=df, x=df['injury'], ax=ax[0])
sns.boxplot(data=df, x=df['injury'], ax=ax[1])

cor_df_num = df_num.corr()
cor_df_num

plt.figure(figsize = (20,8))
sns.heatmap(cor_df_num,annot=True,cmap='Blues')

df.groupby(['Weight','bullied'])[['attempted_suicide']].mean().sort_values('attempted_suicide', ascending = False)

df.groupby(['Age','toxic relationship'])[['attempted_suicide']].mean().sort_values('attempted_suicide', ascending = True)

df.groupby(['Gender','Weight'])[['attempted_suicide']].mean().sort_values('attempted_suicide', ascending = True)

df.groupby(['Gender','friends/Relatives'])[['attempted_suicide']].mean().sort_values('attempted_suicide', ascending = True)

df.groupby(['Gender','drugs'])[['attempted_suicide']].mean().sort_values('attempted_suicide', ascending = True)

df.groupby(['fights','understanding parents'])[['attempted_suicide']].mean().sort_values('attempted_suicide', ascending = True)

"""**FPGROWTH**"""

# Read dataset from binary.csv file
dataset = pd.read_csv('dataa.csv', header=None)
transactions = []

# Convert pandas dataframe to list of transactions
for i in range(len(dataset)):
    transactions.append([str(dataset.values[i,j]) for j in range(len(dataset.columns))])

# Set minimum support count
min_support_count = 300

# Create dictionary to store support count of each item
support_count = defaultdict(int)

# Count the support of each item
for transaction in transactions:
    for item in transaction:
        support_count[item] += 1

# Remove items that don't meet minimum support count
frequent_items = {}
for item, support in support_count.items():
    if support >= min_support_count:
        frequent_items[item] = support

# Sort frequent items by support count
frequent_items = dict(sorted(frequent_items.items(), key=lambda item: item[1], reverse=True))

# Create sorted frequent itemsets from sorted frequent items
sorted_frequent_itemsets = []
for item in frequent_items:
    sorted_frequent_itemsets.append([item])

# Build FP-tree
root = {}
for transaction in transactions:
    transaction = [item for item in transaction if item in frequent_items]
    transaction.sort(key=lambda item: frequent_items[item], reverse=True)
    current_node = root
    for item in transaction:
        if item not in current_node:
            current_node[item] = {'count': 1, 'children': {}}
        else:
            current_node[item]['count'] += 1
        current_node = current_node[item]['children']

# Generate frequent itemsets
frequent_itemsets = []
stack = [{'node': root, 'path': []}]
while len(stack) > 0:
    current_level_items = defaultdict(int)
    while len(stack) > 0:
        current = stack.pop()
        node = current['node']
        path = current['path']
        for item, child_node in node.items():
            current_path = path.copy()
            current_path.append(item)
            current_level_items[tuple(current_path)] += child_node['count']
            stack.append({'node': child_node['children'], 'path': current_path})
    for itemset, count in current_level_items.items():
        if count >= min_support_count:
            frequent_itemsets.append({'items': list(itemset), 'support': count})

# Print frequent itemsets
print("Frequent itemsets (minimum support count = {}):".format(min_support_count))
for itemset in frequent_itemsets:
    print("{} - support count: {}".format(itemset['items'], itemset['support']))

"""**Apriori**"""

# Load dataset
df = pd.read_csv('dataa.csv')

# Convert columns to boolean
df = df.astype(bool)

# Find frequent itemsets using Apriori algorithm
frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1)

# Print frequent itemsets
print(frequent_itemsets)

# Print association rules
print(rules)